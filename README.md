# Interpretable Hybrid Model for Visual Pattern Recognition  
ECE 5831 â€“ Pattern Recognition and Neural Networks  
University of Michiganâ€“Dearborn  


---

## ðŸ“Œ Project Overview

This project investigates *visual pattern recognition* using both *neural networks* and *classical pattern recognition techniques, with a strong emphasis on **model interpretability*.

We begin with a *baseline Convolutional Neural Network (CNN)* trained on the Fashion-MNIST dataset to learn hierarchical visual representations. While CNNs achieve strong classification performance, they often behave as *black-box models*, making it difficult to understand how predictions are formed.

To address this limitation, the project proposes a *hybrid model* that combines:
- *Learned CNN embeddings (deep features)*  
- *Handcrafted visual descriptors* such as *HOG* and *GLCM*

These features are fused and classified using a *Random Forest classifier, enabling both strong performance and enhanced interpretability. This approach aligns directly with core principles of **Pattern Recognition and Neural Networks*.

---

## ðŸŽ¯ Project Objectives

- Train and evaluate a baseline CNN for image classification  
- Analyze CNN behavior using *Grad-CAM, **Saliency Maps, and **Integrated Gradients*  
- Extract handcrafted features for classical pattern recognition  
- Build a hybrid feature-fusion model combining CNN and handcrafted features  
- Improve interpretability using *feature importance analysis* and *SHAP explanations*  
- Compare performance and interpretability between baseline and hybrid models  

---

## ðŸ“Š Dataset

The project uses the *Fashion-MNIST* dataset, a standard benchmark dataset consisting of:
- 28Ã—28 grayscale images  
- 10 clothing categories  
- 60,000 training samples and 10,000 test samples  

The dataset is automatically downloaded using  
torchvision.datasets.FashionMNIST.

Due to size constraints, raw data files and processed tensors are *not included in this GitHub repository*.

---

## How to Run
1. Open final-project.ipynb
2. Run all cells sequentially
3. Generated outputs will appear in the outputs directory


## Links
- ðŸ“½ Presentation Video: https://drive.google.com/drive/folders/1oIV1lOQ4CCR9M3NZtpLtd7MVO26Dq53D
- ðŸ“Š Presentation Slides: https://drive.google.com/drive/folders/1OGMRi0-q6bcvAA3nsQEz0bcmwkvOGZ3v
- ðŸ“„ Final Report: https://drive.google.com/drive/folders/1-HPsyf7017WYr780j8ds9Wcx8TZ9pI1Y
- ðŸ“‚ Dataset: dataset/fashion-mnist.zip
- ðŸŽ¥ Demo Video: https://www.youtube.com/watch?v=PgC6iEznj_M
